var documenterSearchIndex = {"docs":
[{"location":"cga/#Compact-Genetic-Algorithm","page":"CGA","title":"Compact Genetic Algorithm","text":"","category":"section"},{"location":"cga/#cgasample","page":"CGA","title":"cgasample","text":"","category":"section"},{"location":"cga/","page":"CGA","title":"CGA","text":"ErrorsInVariables.cgasample","category":"page"},{"location":"cga/#ErrorsInVariables.CGA.cgasample","page":"CGA","title":"ErrorsInVariables.CGA.cgasample","text":"Generates a binary array of values using a probability vector. Each single element of the probability vector is the probability of bit having  the value of 1. When the probability vector is [1, 1, 1, ..., 1] then the sampled vector is [1.0, 1.0, 1.0, ..., 1.0] whereas it is [0.0, 0.0, 0.0, ..., 0.0] when the probability vector is a vector of zeros. The CGA (compact genetic algorithms) search is started using the  probability vector of [0.5, 0.5, 0.5, ..., 0.5] which produces random vectors of either zeros or ones.\n\nExamples\n\njulia> sample([1, 1, 1, 1, 1])\n5-element Array{Bool,1}:\n 1\n 1\n 1\n 1\n 1\njulia> cgasample(ones(10) * 0.5)\n10-element Array{Bool,1}:\n 1\n 1\n 1\n 1\n 0\n 0\n 0\n 1\n 1\n 0\n\n\n\n\n\n","category":"function"},{"location":"cga/#cga","page":"CGA","title":"cga","text":"","category":"section"},{"location":"cga/","page":"CGA","title":"CGA","text":"ErrorsInVariables.cga","category":"page"},{"location":"cga/#ErrorsInVariables.CGA.cga","page":"CGA","title":"ErrorsInVariables.CGA.cga","text":"Performs a CGA (Compact Genetic Algorithm) search for minimization of an objective function. In the example below, the objective function is to minimize sum of bits of a binary vector. The search method results the optimum vector of [0, 0, ..., 0] where the objective function is zero.\n\nExamples\n\njulia> function f(x)\n           return sum(x)\n       end\nf (generic function with 1 method)\njulia> cga(chsize = 10, costfunction = f, popsize = 100)\n10-element Array{Bool,1}:\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n\n\n\n\n\n","category":"function"},{"location":"estimator/#The-EIVE-CGA-Estimator","page":"The Estimator","title":"The EIVE-CGA Estimator","text":"","category":"section"},{"location":"estimator/#eive","page":"The Estimator","title":"eive","text":"","category":"section"},{"location":"estimator/","page":"The Estimator","title":"The Estimator","text":"ErrorsInVariables.eive","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"(Image: Doc) (Image: codecov)","category":"page"},{"location":"#ErrorsInVariables.jl","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"","category":"section"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"Error-in-variables estimation using Compact Genetic Algorithms in Julia","category":"page"},{"location":"#The-Problem","page":"ErrorsInVariables.jl","title":"The Problem","text":"","category":"section"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"Suppose the linear regression model is ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"\ny = beta_0 + beta_1 x^* + varepsilon\n\n\nwhere yÂ is n-vector of the response variable beta_0 and beta_1 are unknown regression parameteres varepsilon is the iid error term x^* is the unknown n-vector of the independent variable and n is the number of observations\n\nWe call x^* unknown because in some situations the true values of the variable cannot be visible or directly observable or observable with some measurement error Now suppose that x is the observable version of the true values and it is defined as \n\n\nx = x^* + delta\n\n\nwhere delta is the measurement error and x is the erroneous version of the true x^* If the estimated model is \n\n\nhaty = hatbeta_0 + hatbeta_1x \n\n\nthen the ordinary least squares (OLS) estimates are no longer unbiased and even consistent \n\nEive-cga is an estimator devised for this problem The aim is to reduce the errors-in-variable bias with some cost of increasing the variance At the end the estimator obtains lower Mean Square Error (MSE) values defined as\n\n\nMSE(hatbeta_1) = Var(hatbeta_1) + Bias^2(hatbeta_1)\n\n\nfor the Eive-cga estimator For more detailed comparisons see the original paper given in the Citation part \n\n Usage \n\nFor the single variable case \n\nJulia \njulia eive(dirtyx = dirtyx y = y otherx = nothing) \n\n\nand for the multiple regression \n\nJulia \njulia eive(dirtyx = dirtyx y = y otherx = matrixofotherx) \n\n\nNote that the method assumes there is only one erroneous variable in the set of independent variables\n\n Example \n\nLets generate data from the model y = 20 + 10x^* + varepsilon\n\njulia\nimport Random\nusing ErrorsInVariables\n\nrng = RandomMersenneTwister(1234)\nn = 30\ndeltax = randn(rng n) * sqrt(30)\ncleanx = randn(rng n) * sqrt(70)\ne = randn(rng n) * sqrt(50)\ny = 200 + 100 * cleanx + e\ndirtyx = cleanx\n\n\nand assume that \n\n\nx^*\n \n\nis unobservable and it is observed as \n\n\nx = x^* + delta","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":". We can calculate an unbiased estimate of the slope parameter using ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"eive(dirtyx = dirtyx, y = y, otherx = nothing) ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"The result is ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"EiveResult([20.28458307772922, 9.456757289676714])","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"whereas OLS estimates are","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"julia> X = hcat(ones(n), dirtyx);\n\njulia> X \\ y\n2-element Vector{Float64}:\n 17.94867860059858\n  5.8099584879737876","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"and clearly biased towards to zero.","category":"page"},{"location":"#Citation","page":"ErrorsInVariables.jl","title":"Citation","text":"","category":"section"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"@article{satman2015reducing,\n  title={Reducing errors-in-variables bias in linear regression using compact genetic algorithms},\n  author={Satman, M Hakan and Diyarbakirlioglu, Erkin},\n  journal={Journal of Statistical Computation and Simulation},\n  volume={85},\n  number={16},\n  pages={3216--3235},\n  year={2015},\n  publisher={Taylor \\& Francis}\n}","category":"page"}]
}
