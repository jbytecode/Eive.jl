var documenterSearchIndex = {"docs":
[{"location":"cga/#Compact-Genetic-Algorithm","page":"CGA","title":"Compact Genetic Algorithm","text":"","category":"section"},{"location":"cga/#cgasample","page":"CGA","title":"cgasample","text":"","category":"section"},{"location":"cga/","page":"CGA","title":"CGA","text":"ErrorsInVariables.cgasample","category":"page"},{"location":"cga/#ErrorsInVariables.CGA.cgasample","page":"CGA","title":"ErrorsInVariables.CGA.cgasample","text":"Generates a binary vector of values using a probability vector. Each single element of the probability vector is the probability of bit having  the value of 1. When the probability vector is [1, 1, 1, ..., 1] then the sampled vector is [1.0, 1.0, 1.0, ..., 1.0] whereas it is [0.0, 0.0, 0.0, ..., 0.0] when the probability vector is a vector of zeros. The CGA (compact genetic algorithms) search is started using the  probability vector of [0.5, 0.5, 0.5, ..., 0.5] which produces random vectors of either zeros or ones.\n\nExamples\n\njulia> sample([1, 1, 1, 1, 1])\n5-element Vector{Int}:\n 1\n 1\n 1\n 1\n 1\njulia> cgasample(ones(10) * 0.5)\n10-element Vector{Int}:\n 1\n 1\n 1\n 1\n 0\n 0\n 0\n 1\n 1\n 0\n\n\n\n\n\n","category":"function"},{"location":"cga/#cga","page":"CGA","title":"cga","text":"","category":"section"},{"location":"cga/","page":"CGA","title":"CGA","text":"ErrorsInVariables.cga","category":"page"},{"location":"cga/#ErrorsInVariables.CGA.cga","page":"CGA","title":"ErrorsInVariables.CGA.cga","text":"Performs a CGA (Compact Genetic Algorithm) search for minimization of an objective function. In the example below, the objective function is to minimize sum of bits of a binary vector. The search method results the optimum vector of [0, 0, ..., 0] where the objective function is zero.\n\nExamples\n\njulia> function f(x)\n\t\t   return sum(x)\n\t   end\nf (generic function with 1 method)\njulia> cga(chsize = 10, costfunction = f, popsize = 100)\n10-element Vector{Int}:\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n\n\n\n\n\n","category":"function"},{"location":"estimator/#The-EIVE-CGA-Estimator","page":"The Estimator","title":"The EIVE-CGA Estimator","text":"","category":"section"},{"location":"estimator/#eive","page":"The Estimator","title":"eive","text":"","category":"section"},{"location":"estimator/","page":"The Estimator","title":"The Estimator","text":"ErrorsInVariables.eive","category":"page"},{"location":"estimator/#ErrorsInVariables.Estimator.eive","page":"The Estimator","title":"ErrorsInVariables.Estimator.eive","text":"eive(;\ndirtyx::Vector{T},\ny::Vector{T},\notherx::Union{Nothing, Matrix{T}, Vector{T}},\npopsize::Int = 50,\nnumdummies::Int = 10,\nrng::RNGType = MersenneTwister(1234)\n\n)::EiveResult where {T<:Real, RNGType<:AbstractRNG}\n\nDescription:\n\nThe method searches for a set of dummy (binary) variables that separates the erroneous independent variable into clean part and error part. The clean part is then used in the main regression estimation.  Those dummy variables minimize the sum of squares of residuals of the main regression. In other terms the methods searches for a set of proxy variables that do not exist in real. Please see the reference for details.  \n\nArguments:\n\ndirtyx: Independent variable measured with some error\ny: Dependent variable\notherx: Matrix of other independent variables\npopsize: Number of individuals in the population (optional)\nnumdummies: Number of dummy variables to use (optional)\nrng: Random number generator (optional)\n\nExamples\n\njulia> import Random\njulia> using ErrorsInVariables\njulia> rng = Random.MersenneTwister(1234)\njulia> n = 30\njulia> deltax = randn(rng, n) * sqrt(3.0)\njulia> cleanx = randn(rng, n) * sqrt(7.0)\njulia> e = randn(rng, n) * sqrt(5.0)\njulia> y = 20.0 .+ 10.0 .* cleanx .+ e\njulia> dirtyx = cleanx + deltax\njulia> eive(dirtyx = dirtyx, y = y, otherx = nothing) \n\nEiveResult([20.28458307772922, 9.456757289676714])\n\njulia> X = hcat(ones(n), dirtyx);\n\njulia> # Biased OLS estimates:\njulia> X \\ y\n2-element Vector{Float64}:\n 17.94867860059858\n  5.8099584879737876\n\nReferences\n\nSatman, M. Hakan, and Erkin Diyarbakirlioglu. \"Reducing errors-in-variables bias in linear  regression using compact genetic algorithms.\" Journal of Statistical Computation and Simulation  85.16 (2015): 3216-3235.\n\n\n\n\n\n","category":"function"},{"location":"orthogonalregression/#Orthogonal-Regression","page":"Orthogonal Regression","title":"Orthogonal Regression","text":"","category":"section"},{"location":"orthogonalregression/#orthogonal_regression","page":"Orthogonal Regression","title":"orthogonal_regression","text":"","category":"section"},{"location":"orthogonalregression/","page":"Orthogonal Regression","title":"Orthogonal Regression","text":"ErrorsInVariables.orthogonal_regression","category":"page"},{"location":"orthogonalregression/#ErrorsInVariables.OrthogonalRegression.orthogonal_regression","page":"Orthogonal Regression","title":"ErrorsInVariables.OrthogonalRegression.orthogonal_regression","text":"orthogonal_regression(X::Matrix,\ny::Vector,\nxhasintercept::Bool=true,\nmaxiterations::Int=10000,\ninitialbetas::Union{Nothing, Vector} = nothing)::EiveResult\n\nDescription:\n\nThe function performs orthogonal regression. The method searches for a set of  parameters that minimize the sum of squares of orthogonal residuals.\n\nArguments:\n\nX::Matrix: Independent variables. The first column should be 1 for the intercept. y::Vector: Dependent variable. xhasintercept::Bool: If true, the first column of X is considered as the intercept. maxiterations::Int: Maximum number of iterations. initialbetas::Union{Nothing, Vector}: Initial values for the parameters.  If nothing, random values are used.\n\nExamples\n\nX = Float64[1 1.0; 1 0.6; 1 1.2; 1 1.4; 1 0.2]\ny = Float64[0.5, 0.3, 0.7, 1.0, 0.2]\n\nresult = orthogonal_regression(X, y)\n\nReferences\n\nhttps://davegiles.blogspot.com/2014/11/orthogonal-regression-first-steps.html\n\n\n\n\n\n","category":"function"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"(Image: Doc) (Image: codecov)","category":"page"},{"location":"#ErrorsInVariables.jl","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"","category":"section"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"Error-in-variables estimation using Compact Genetic Algorithms in Julia","category":"page"},{"location":"#Usage","page":"ErrorsInVariables.jl","title":"Usage","text":"","category":"section"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"For the single variable case ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"julia> eive(dirtyx = dirtyx, y = y, otherx = nothing) ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"and for the multiple regression ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"julia> eive(dirtyx = dirtyx, y = y, otherx = matrixofotherx) ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"Note that the method assumes there is only one erroneous variable in the set of independent variables.","category":"page"},{"location":"#Example","page":"ErrorsInVariables.jl","title":"Example","text":"","category":"section"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"Lets generate data from the model y = 20 + 10x^* + varepsilon","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"import Random\nusing ErrorsInVariables\n\nrng = Random.MersenneTwister(1234)\nn = 30\ndeltax = randn(rng, n) * sqrt(3.0)\ncleanx = randn(rng, n) * sqrt(7.0)\ne = randn(rng, n) * sqrt(5.0)\ny = 20.0 .+ 10.0 .* cleanx .+ e\ndirtyx = cleanx\neive(dirtyx = dirtyx, y = y, otherx = nothing) ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"The result is ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"EiveResult([20.28458307772922, 9.456757289676714])","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"whereas OLS estimates are","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"julia> X = hcat(ones(n), dirtyx);\n\njulia> X \\ y\n2-element Vector{Float64}:\n 17.94867860059858\n  5.8099584879737876","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"and clearly biased towards to zero.","category":"page"},{"location":"#Citation","page":"ErrorsInVariables.jl","title":"Citation","text":"","category":"section"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"@article{satman2015reducing,\n  title={Reducing errors-in-variables bias in linear regression using compact genetic algorithms},\n  author={Satman, M Hakan and Diyarbakirlioglu, Erkin},\n  journal={Journal of Statistical Computation and Simulation},\n  volume={85},\n  number={16},\n  pages={3216--3235},\n  year={2015},\n  publisher={Taylor \\& Francis}\n}\n\n@article{Satman_Diyarbakirlioglu_2024, \ntitle={A Solution to Errors-in-variables Bias in Multivariate Linear Regression using Compact Genetic Algorithms},\nvolume={4}, \nurl={https://journals.gen.tr/index.php/jame/article/view/2293}, \nDOI={10.53753/jame.2293}, \nnumber={1}, \njournal={JOURNAL OF APPLIED MICROECONOMETRICS}, \nauthor={Satman, Mehmet Hakan and Diyarbakırlıoğlu, Erkin}, \nyear={2024}, month={Jun.}, \npages={31–64} \n}","category":"page"},{"location":"multivariate/#The-EIVE-CGA-Estimator-for-Multivariate-Case","page":"Multivariate Case","title":"The EIVE-CGA Estimator for Multivariate Case","text":"","category":"section"},{"location":"multivariate/#eive","page":"Multivariate Case","title":"eive","text":"","category":"section"},{"location":"multivariate/","page":"Multivariate Case","title":"Multivariate Case","text":"ErrorsInVariables.meive","category":"page"},{"location":"multivariate/#ErrorsInVariables.Eivem.meive","page":"Multivariate Case","title":"ErrorsInVariables.Eivem.meive","text":"eive(;\ndirtyx::Vector{T},\ny::Matrix{T},\notherx::Union{Nothing, Matrix{T}, Vector{T}},\npopsize::Int = 50,\nnumdummies::Int = 10,\nrng::RNGType = MersenneTwister(1234)\n\n)::EiveResult where {T<:Real, RNGType<:AbstractRNG}\n\nDescription:\n\nThis is the multivariate case of eive(). Please see eive() function.  In the multivariate case, the y is not vector, but a matrix of multiple  or repeated measurements of the response variable. This can be considered  as multivariate regression as well as regressions with repeated measurements.  \n\nArguments:\n\ndirtyx: Independent variable measured with some error\ny: nxp matrix of dependent variables where n is the number of observations and p is the number of dependent variables\notherx: Matrix of other independent variables\npopsize: Number of individuals in the population (optional)\nnumdummies: Number of dummy variables to use (optional)\nrng: Random number generator (optional)\n\nExamples\n\njulia> import Random\njulia> using ErrorsInVariables\njulia> rng = Random.MersenneTwister(1234)\njulia> n = 30\njulia> deltax = randn(rng, n) * sqrt(3.0)\njulia> cleanx = randn(rng, n) * sqrt(7.0)\njulia> e1 = randn(rng, n) * sqrt(5.0)\njulia> e2 = randn(rng, n) * sqrt(5.0)\njulia> y1 = 20.0 .+ 10.0 .* cleanx .+ e1\njulia> y2 = 10.0 .+ 15.0 .* cleanx .+ e2\njulia> dirtyx = cleanx + deltax\n\njulia> # Getting bias-reduced estimates\njulia> meive(dirtyx = dirtyx, y = hcat(y1, y2), otherx = nothing) \n\nEiveResult([19.65449584842238, 9.21108792897651])\n\njulia> X = hcat(ones(n), dirtyx);\n\njulia> # Biased OLS estimates:\njulia> X \\ y1\n2-element Vector{Float64}:\n17.94867860059858\n  5.8099584879737876\n\nReferences\n\nSatman, M. Hakan, and Erkin Diyarbakirlioglu. \"Reducing errors-in-variables bias in linear  regression using compact genetic algorithms.\" Journal of Statistical Computation and Simulation  85.16 (2015): 3216-3235.\n\nSatman, M. H., & Diyarbakırlıoğlu, E. (2024). A Solution to Errors-in-variables Bias in Multivariate  Linear Regression using Compact Genetic Algorithms. JOURNAL OF APPLIED MICROECONOMETRICS,  4(1), 31–64. https://doi.org/10.53753/jame.2293\n\n\n\n\n\n","category":"function"}]
}
