var documenterSearchIndex = {"docs":
[{"location":"cga/#Compact-Genetic-Algorithm","page":"CGA","title":"Compact Genetic Algorithm","text":"","category":"section"},{"location":"cga/#cgasample","page":"CGA","title":"cgasample","text":"","category":"section"},{"location":"cga/","page":"CGA","title":"CGA","text":"ErrorsInVariables.cgasample","category":"page"},{"location":"cga/#ErrorsInVariables.CGA.cgasample","page":"CGA","title":"ErrorsInVariables.CGA.cgasample","text":"Generates a binary vector of values using a probability vector. Each single element of the probability vector is the probability of bit having  the value of 1. When the probability vector is [1, 1, 1, ..., 1] then the sampled vector is [1.0, 1.0, 1.0, ..., 1.0] whereas it is [0.0, 0.0, 0.0, ..., 0.0] when the probability vector is a vector of zeros. The CGA (compact genetic algorithms) search is started using the  probability vector of [0.5, 0.5, 0.5, ..., 0.5] which produces random vectors of either zeros or ones.\n\nExamples\n\njulia> sample([1, 1, 1, 1, 1])\n5-element Vector{Int}:\n 1\n 1\n 1\n 1\n 1\njulia> cgasample(ones(10) * 0.5)\n10-element Vector{Int}:\n 1\n 1\n 1\n 1\n 0\n 0\n 0\n 1\n 1\n 0\n\n\n\n\n\n","category":"function"},{"location":"cga/#cga","page":"CGA","title":"cga","text":"","category":"section"},{"location":"cga/","page":"CGA","title":"CGA","text":"ErrorsInVariables.cga","category":"page"},{"location":"cga/#ErrorsInVariables.CGA.cga","page":"CGA","title":"ErrorsInVariables.CGA.cga","text":"Performs a CGA (Compact Genetic Algorithm) search for minimization of an objective function. In the example below, the objective function is to minimize sum of bits of a binary vector. The search method results the optimum vector of [0, 0, ..., 0] where the objective function is zero.\n\nExamples\n\njulia> function f(x)\n\t\t   return sum(x)\n\t   end\nf (generic function with 1 method)\njulia> cga(chsize = 10, costfunction = f, popsize = 100)\n10-element Vector{Int}:\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n\n\n\n\n\n","category":"function"},{"location":"simex/#Simulation-Extrapolation","page":"Simulation Extrapolation","title":"Simulation Extrapolation","text":"","category":"section"},{"location":"simex/#simex","page":"Simulation Extrapolation","title":"simex","text":"","category":"section"},{"location":"simex/","page":"Simulation Extrapolation","title":"Simulation Extrapolation","text":"ErrorsInVariables.simex","category":"page"},{"location":"simex/#ErrorsInVariables.SimulationExtrapolation.simex","page":"Simulation Extrapolation","title":"ErrorsInVariables.SimulationExtrapolation.simex","text":"simex(\nX::Matrix,\ny::Vector,\nλ::Vector,\nerrorvarindex::Int,\nerrvariance::Float64;\nnumsims::Int = 1000)::SimpleEiveResult\n\nDescription\n\nThis function implements the Simulation Extrapolation (SIMEX) method.  The function fits a linear model to the data and then extrapolates the results  to a new value of the parameter λ. The function returns the estimated parameters  and a boolean indicating if the optimization converged.\n\nArguments\n\nX::Matrix: A matrix of the independent variables.\ny::Vector: A vector of the dependent variable.\nλ::Vector: A vector of the values of the parameter λ.\nerrorvarindex::Int: The index of the error variable in the X matrix.\nerrvariance::Float64: The variance of the error variable.\nnumsims::Int = 1000: The number of simulations to run.\n\nReturns\n\nSimpleEiveResult: An object that contains the estimated parameters of the model.\n\nReferences\n\nCook, John R., and Leonard A. Stefanski. \"Simulation-extrapolation estimation in parametric \n\nmeasurement error models.\" Journal of the American Statistical association 89.428 (1994): 1314-1328.\n\n\n\n\n\n","category":"function"},{"location":"simex/#simex*single*iteration","page":"Simulation Extrapolation","title":"simexsingleiteration","text":"","category":"section"},{"location":"simex/","page":"Simulation Extrapolation","title":"Simulation Extrapolation","text":"ErrorsInVariables.SimulationExtrapolation.simex_single_iteration","category":"page"},{"location":"simex/#ErrorsInVariables.SimulationExtrapolation.simex_single_iteration","page":"Simulation Extrapolation","title":"ErrorsInVariables.SimulationExtrapolation.simex_single_iteration","text":"simex_single_iteration(\n    X::Matrix, \n    y::Vector, \n    λ::Float64, \n    errorvarindex::Int, \n    errvariance::Float64; \n    numsims::Int = 1000)::Vector\n\nDescription\n\nThis function performs a single iteration of the Simulation Extrapolation (SIMEX) method.\n\nArguments\n\nX::Matrix: A matrix of the independent variables.\ny::Vector: A vector of the dependent variable.\nλ::Float64: The value of the parameter λ.\nerrorvarindex::Int: The index of the error variable in the X matrix.\nerrvariance::Float64: The variance of the error variable.\nnumsims::Int = 1000: The number of simulations to run.\n\nReturns\n\nVector: A vector of the estimated parameters of the model.\n\n\n\n\n\n","category":"function"},{"location":"simex/#simex*multiple*iterations","page":"Simulation Extrapolation","title":"simexmultipleiterations","text":"","category":"section"},{"location":"simex/","page":"Simulation Extrapolation","title":"Simulation Extrapolation","text":"ErrorsInVariables.SimulationExtrapolation.simex_multiple_iterations","category":"page"},{"location":"simex/#ErrorsInVariables.SimulationExtrapolation.simex_multiple_iterations","page":"Simulation Extrapolation","title":"ErrorsInVariables.SimulationExtrapolation.simex_multiple_iterations","text":"simex_multiple_iterations(\n    X::Matrix, \n    y::Vector, \n    λ::Vector, \n    errorvarindex::Int, \n    errvariance::Float64; \n    numsims::Int = 1000\n)::Matrix\n\nDescription\n\nThis function performs multiple iterations of the Simulation Extrapolation (SIMEX) method.\n\nArguments\n\nX::Matrix: A matrix of the independent variables.\ny::Vector: A vector of the dependent variable.\nλ::Vector: A vector of the values of the parameter λ.\nerrorvarindex::Int: The index of the error variable in the X matrix.\nerrvariance::Float64: The variance of the error variable.\nnumsims::Int = 1000: The number of simulations to run.\n\nReturns\n\nMatrix: A matrix of the estimated parameters of the model.\n\n\n\n\n\n","category":"function"},{"location":"simex/#extrapolate","page":"Simulation Extrapolation","title":"extrapolate","text":"","category":"section"},{"location":"simex/","page":"Simulation Extrapolation","title":"Simulation Extrapolation","text":"ErrorsInVariables.SimulationExtrapolation.extrapolate","category":"page"},{"location":"simex/#ErrorsInVariables.SimulationExtrapolation.extrapolate","page":"Simulation Extrapolation","title":"ErrorsInVariables.SimulationExtrapolation.extrapolate","text":"extrapolate(λ::Vector, betas::Matrix, errorvarindex::Int)::Float64\n\nDescription\n\nThis function extrapolates the results of the Simulation Extrapolation (SIMEX) method to  a new value of the parameter λ = -1.\n\nArguments\n\nλ::Vector: A vector of the values of the parameter λ.\nbetas::Matrix: A matrix of the estimated parameters for each value of λ.\nerrorvarindex::Int: The index of the error variable in the X matrix.\n\nReturns\n\nFloat64: The extrapolated value of the parameter.\n\nReferences\n\nCook, John R., and Leonard A. Stefanski. \"Simulation-extrapolation estimation in parametric \n\nmeasurement error models.\" Journal of the American Statistical association 89.428 (1994): 1314-1328.\n\n\n\n\n\n","category":"function"},{"location":"estimator/#The-EIVE-CGA-Estimator","page":"The Estimator","title":"The EIVE-CGA Estimator","text":"","category":"section"},{"location":"estimator/#eive","page":"The Estimator","title":"eive","text":"","category":"section"},{"location":"estimator/","page":"The Estimator","title":"The Estimator","text":"ErrorsInVariables.eive","category":"page"},{"location":"estimator/#ErrorsInVariables.Estimator.eive","page":"The Estimator","title":"ErrorsInVariables.Estimator.eive","text":"eive(;\ndirtyx::Vector{T},\ny::Vector{T},\notherx::Union{Nothing, Matrix{T}, Vector{T}},\npopsize::Int = 50,\nnumdummies::Int = 10,\nrng::RNGType = MersenneTwister(1234))::SimpleEiveResult where {T<:Real, RNGType<:AbstractRNG}\n\nDescription:\n\nThe method searches for a set of dummy (binary) variables that separates the erroneous independent variable into clean part and error part. The clean part is then used in the main regression estimation.  Those dummy variables minimize the sum of squares of residuals of the main regression. In other terms the methods searches for a set of proxy variables that do not exist in real. Please see the reference for details.  \n\nArguments:\n\ndirtyx: Independent variable measured with some error\ny: Dependent variable\notherx: Matrix of other independent variables\npopsize: Number of individuals in the population (optional)\nnumdummies: Number of dummy variables to use (optional)\nrng: Random number generator (optional)\n\nExamples\n\njulia> import Random\njulia> using ErrorsInVariables\njulia> rng = Random.MersenneTwister(1234)\njulia> n = 30\njulia> deltax = randn(rng, n) * sqrt(3.0)\njulia> cleanx = randn(rng, n) * sqrt(7.0)\njulia> e = randn(rng, n) * sqrt(5.0)\njulia> y = 20.0 .+ 10.0 .* cleanx .+ e\njulia> dirtyx = cleanx + deltax\njulia> eive(dirtyx = dirtyx, y = y, otherx = nothing) \n\nEiveResult([20.28458307772922, 9.456757289676714])\n\njulia> X = hcat(ones(n), dirtyx);\n\njulia> # Biased OLS estimates:\njulia> X \\ y\n2-element Vector{Float64}:\n 17.94867860059858\n  5.8099584879737876\n\nReferences\n\nSatman, M. Hakan, and Erkin Diyarbakirlioglu. \"Reducing errors-in-variables bias in linear  regression using compact genetic algorithms.\" Journal of Statistical Computation and Simulation  85.16 (2015): 3216-3235.\n\n\n\n\n\n","category":"function"},{"location":"orthogonalregression/#Orthogonal-Regression","page":"Orthogonal Regression","title":"Orthogonal Regression","text":"","category":"section"},{"location":"orthogonalregression/#orthogonal_regression","page":"Orthogonal Regression","title":"orthogonal_regression","text":"","category":"section"},{"location":"orthogonalregression/","page":"Orthogonal Regression","title":"Orthogonal Regression","text":"ErrorsInVariables.orthogonal_regression","category":"page"},{"location":"orthogonalregression/#ErrorsInVariables.OrthogonalRegression.orthogonal_regression","page":"Orthogonal Regression","title":"ErrorsInVariables.OrthogonalRegression.orthogonal_regression","text":"orthogonal_regression(\n    X::Matrix,\n    y::Vector;\n    xhasintercept::Bool=true,\n    maxiterations::Int=10000,\n    initialbetas::Union{Nothing, Vector} = nothing)::SimpleEiveResult\n\nDescription:\n\nThe function performs orthogonal regression. The method searches for a set of  parameters that minimize the sum of squares of orthogonal residuals.\n\nArguments:\n\nX::Matrix: Independent variables. The first column should be 1 for the intercept.\n\ny::Vector: Dependent variable.\n\nxhasintercept::Bool: If true, the first column of X is considered as the intercept.\n\nmaxiterations::Int: Maximum number of iterations.\n\ninitialbetas::Union{Nothing, Vector}: Initial values for the parameters.  If nothing, random values are used.\n\nReturns:\n\n::EiveResult: A struct containing the estimated parameters and convergence status.\n\nExamples\n\nX = Float64[1 1.0; 1 0.6; 1 1.2; 1 1.4; 1 0.2]\ny = Float64[0.5, 0.3, 0.7, 1.0, 0.2]\n\nresult = orthogonal_regression(X, y)\n\nReferences\n\nhttps://davegiles.blogspot.com/2014/11/orthogonal-regression-first-steps.html\n\n\n\n\n\n","category":"function"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"(Image: Doc) (Image: codecov)","category":"page"},{"location":"#ErrorsInVariables.jl","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"","category":"section"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"Error-in-variables estimation using Compact Genetic Algorithms in Julia","category":"page"},{"location":"#Usage","page":"ErrorsInVariables.jl","title":"Usage","text":"","category":"section"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"For the single variable case ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"julia> eive(dirtyx = dirtyx, y = y, otherx = nothing) ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"and for the multiple regression ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"julia> eive(dirtyx = dirtyx, y = y, otherx = matrixofotherx) ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"Note that the method assumes there is only one erroneous variable in the set of independent variables.","category":"page"},{"location":"#Example","page":"ErrorsInVariables.jl","title":"Example","text":"","category":"section"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"Lets generate data from the model y = 20 + 10x^* + varepsilon","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"import Random\nusing ErrorsInVariables\n\nrng = Random.MersenneTwister(1234)\nn = 30\ndeltax = randn(rng, n) * sqrt(3.0)\ncleanx = randn(rng, n) * sqrt(7.0)\ne = randn(rng, n) * sqrt(5.0)\ny = 20.0 .+ 10.0 .* cleanx .+ e\ndirtyx = cleanx\neive(dirtyx = dirtyx, y = y, otherx = nothing) ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"The result is ","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"EiveResult([20.28458307772922, 9.456757289676714])","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"whereas OLS estimates are","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"julia> X = hcat(ones(n), dirtyx);\n\njulia> X \\ y\n2-element Vector{Float64}:\n 17.94867860059858\n  5.8099584879737876","category":"page"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"and clearly biased towards to zero.","category":"page"},{"location":"#Citation","page":"ErrorsInVariables.jl","title":"Citation","text":"","category":"section"},{"location":"","page":"ErrorsInVariables.jl","title":"ErrorsInVariables.jl","text":"@article{satman2015reducing,\n  title={Reducing errors-in-variables bias in linear regression using compact genetic algorithms},\n  author={Satman, M Hakan and Diyarbakirlioglu, Erkin},\n  journal={Journal of Statistical Computation and Simulation},\n  volume={85},\n  number={16},\n  pages={3216--3235},\n  year={2015},\n  publisher={Taylor \\& Francis}\n}\n\n@article{Satman_Diyarbakirlioglu_2024, \ntitle={A Solution to Errors-in-variables Bias in Multivariate Linear Regression using Compact Genetic Algorithms},\nvolume={4}, \nurl={https://journals.gen.tr/index.php/jame/article/view/2293}, \nDOI={10.53753/jame.2293}, \nnumber={1}, \njournal={JOURNAL OF APPLIED MICROECONOMETRICS}, \nauthor={Satman, Mehmet Hakan and Diyarbakırlıoğlu, Erkin}, \nyear={2024}, month={Jun.}, \npages={31–64} \n}","category":"page"},{"location":"deming/#Deming-Regression","page":"Deming Regression","title":"Deming Regression","text":"","category":"section"},{"location":"deming/#deming","page":"Deming Regression","title":"deming","text":"","category":"section"},{"location":"deming/","page":"Deming Regression","title":"Deming Regression","text":"ErrorsInVariables.deming","category":"page"},{"location":"deming/#ErrorsInVariables.DemingRegression.deming","page":"Deming Regression","title":"ErrorsInVariables.DemingRegression.deming","text":"deming(x::Vector, y::Vector, λ::Float64 = 1.0)::SimpleEiveResult\n\nDescription\n\nThis function estimates the parameters of a linear model using the Deming regression method.\n\nArguments\n\nx::Vector: A vector of the independent variable.\ny::Vector: A vector of the dependent variable.\nλ::Float64 = 1.0: The ratio of the variance of the errors in the y variable to the variance of the errors in the x variable.\n\nReturns\n\nEiveResult: An object that contains the estimated parameters of the model.\n\nExample\n\nusing ErrorsInVariables\n\nx = Float64[7, 8.3, 10.5, 9, 5.1, 8.2, 10.2, 10.3, 7.1, 5.9]\ny = Float64[7.9, 8.2, 9.6, 9, 6.5, 7.3, 10.2, 10.6, 6.3, 5.2]\n\n# The lambda parameter is set to 4.0\n# Lambda is the ratio of the variance of the errors in the y variable \n# to the variance of the errors in the x variable.\nλ = 1.0 / 4.0\n\nresult = deming(x, y, λ)\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Deming_regression\n\n\n\n\n\n","category":"function"},{"location":"multivariate/#The-EIVE-CGA-Estimator-for-Multivariate-Case","page":"Multivariate Case","title":"The EIVE-CGA Estimator for Multivariate Case","text":"","category":"section"},{"location":"multivariate/#eive","page":"Multivariate Case","title":"eive","text":"","category":"section"},{"location":"multivariate/","page":"Multivariate Case","title":"Multivariate Case","text":"ErrorsInVariables.meive","category":"page"},{"location":"multivariate/#ErrorsInVariables.Eivem.meive","page":"Multivariate Case","title":"ErrorsInVariables.Eivem.meive","text":"eive(;\ndirtyx::Vector{T},\ny::Matrix{T},\notherx::Union{Nothing, Matrix{T}, Vector{T}},\npopsize::Int = 50,\nnumdummies::Int = 10,\nrng::RNGType = MersenneTwister(1234))::SimpleEiveResult where {T<:Real, RNGType<:AbstractRNG}\n\nDescription:\n\nThis is the multivariate case of eive(). Please see eive() function.  In the multivariate case, the y is not vector, but a matrix of multiple  or repeated measurements of the response variable. This can be considered  as multivariate regression as well as regressions with repeated measurements.  \n\nArguments:\n\ndirtyx: Independent variable measured with some error\ny: nxp matrix of dependent variables where n is the number of observations and p is the number of dependent variables\notherx: Matrix of other independent variables\npopsize: Number of individuals in the population (optional)\nnumdummies: Number of dummy variables to use (optional)\nrng: Random number generator (optional)\n\nExamples\n\njulia> import Random\njulia> using ErrorsInVariables\njulia> rng = Random.MersenneTwister(1234)\njulia> n = 30\njulia> deltax = randn(rng, n) * sqrt(3.0)\njulia> cleanx = randn(rng, n) * sqrt(7.0)\njulia> e1 = randn(rng, n) * sqrt(5.0)\njulia> e2 = randn(rng, n) * sqrt(5.0)\njulia> y1 = 20.0 .+ 10.0 .* cleanx .+ e1\njulia> y2 = 10.0 .+ 15.0 .* cleanx .+ e2\njulia> dirtyx = cleanx + deltax\n\njulia> # Getting bias-reduced estimates\njulia> meive(dirtyx = dirtyx, y = hcat(y1, y2), otherx = nothing) \n\nEiveResult([19.65449584842238, 9.21108792897651])\n\njulia> X = hcat(ones(n), dirtyx);\n\njulia> # Biased OLS estimates:\njulia> X \\ y1\n2-element Vector{Float64}:\n17.94867860059858\n  5.8099584879737876\n\nReferences\n\nSatman, M. Hakan, and Erkin Diyarbakirlioglu. \"Reducing errors-in-variables bias in linear  regression using compact genetic algorithms.\" Journal of Statistical Computation and Simulation  85.16 (2015): 3216-3235.\n\nSatman, M. H., & Diyarbakırlıoğlu, E. (2024). A Solution to Errors-in-variables Bias in Multivariate  Linear Regression using Compact Genetic Algorithms. JOURNAL OF APPLIED MICROECONOMETRICS,  4(1), 31-64. https://doi.org/10.53753/jame.2293\n\n\n\n\n\n","category":"function"}]
}
